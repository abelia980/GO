# ML_basic_concept

### 监督式机器学习

机器学习系统通过学习如何组合输入信息来对从未见过的数据做出有用的预测

### 标签

标签是我们要预测的事物，即简单线性回归中的y变量。

### 特征

特征是输入变量，即简单线性回归中的x变量，一般用​如下方式指定：
$$
X_1,X_2,X_3,...X_N
$$
<!--合适的特征应该是具体且可量化的-->

### 样本

样本是指数据的特定实例：**x**（采用粗体的x来表示它是一个矢量），将样本分为以下两类：有标签样本和无标签样本。

### 模型

模型定义了特征与标签之间的关系。例如：垃圾邮件检测模型可能会将某些特征与“垃圾邮件”紧密联系起来。

<!--这里说的很对，模型不单单指数学模型，还有语言模型，因此说关系是最能表达特征与标签之间联系的词语-->

#### 模型生命周期的两个阶段

**训练**：是指创建或者学习模型。向模型展示有标签样本，让模型逐渐学习特征与标签之间的关系。<!--通过有标签样本来学习所有权重和偏差的理想值，经验风险最小化-->

**推断**：是指将训练后的模型应用于无标签的样本。

### 回归与分类

**回归**：可以预测连续值

**分类**：可以预测离散值

### 损失函数

#### 平方损失（$L_2$损失）

均方误差（**MSE**）：指每一个样本的平均平方损失，计算各个样本的所有平方损失之和，然后除以样本数量,MSE的计算公式如下:
$$
MSE=\frac{1}{N}\sum_{(x,y)\in D }(y-prediction(x))^2
$$

其中，$(x,y)$指的是样本：

- $x$指的是模型进行预测时使用的特征集
- $y$指的是样本的标签
- $prediction(x)$指的是权重和偏差与特征集$x$结合的函数
- $D$指的是包含多个有标签样本的数据集
- $N$是指D中的样本数量

​	<!--虽然MSE常用于机器学习，但他既不是唯一实用的损失函数，也不是适用于所有情形的最佳损失函数-->

### 降低损失

#### 梯度

梯度是偏导数的矢量，它可以让您了解哪一个方向距离目标“更近”或者“更远”，对于单个权重的梯度就等于导数。梯度是一个矢量，所以具有如下两个特征：**方向**、**大小**。梯度始终指向损失函数中增长最为迅猛的方向，梯度下降算法会沿着负梯度的方向走一步。

#### 批量

批量指的是用于单次迭代中计算梯度的样本总数。包含随机抽样样本的大型数据集可能包含冗余数据，实际上，批量大小越大，出现冗余的可能性就越高。**一些冗余可能有助于消除杂乱的梯度**。

#### 随机梯度下降（SGD）

它每一次迭代只使用一个样本，如果进行足够的迭代，SGD也可以发挥作用，但是过程会**非常杂乱。**

### 过拟合（Generalization）

机器学习的基本冲突是适当拟合我们的数据，但是也要尽可能的简单地拟合数据。机器学习的目标四：**对从真实概率分布（隐藏，未知）中抽取的新数据做出良好的预测，但恰恰是因为它未知，所以模型无法查看整体情况，而只能从训练集中取样**。

奥卡姆剃刀定律在ML中运用如下：

> 机器学习模型越简单，良好的实证结果就越有可能不仅仅基于样本的特性

模型泛化到新数据的能力：

- 模型的复杂程度
- 模型在处理训练数据方面的表现

机器学习细则

- 从分布中随机抽取**独立同分布**（i，i，d）的样本
- 分布是**平稳**的，分布在数据集内不会发生变化
- 我们从同一分布的数据中抽取样本

### 测试集训练集验证集

三种数据集的效果：

1. 选择在验证集上获得最佳效果的模型
2. 使用测试集再次检查模型

<!--该工作流程更好，因为它暴露给测试集的信息更少-->