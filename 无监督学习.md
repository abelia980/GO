### 无监督学习

#### K-means聚类

- k-means聚类有以下三步：
  -  chooses the initial centroids randomly
  - assign each sample to its nearest centroid.
  - creates new centroids by taking the mean value of all the samples 

  直到所有类别的质心与类别内其他点的距离之和小于某个阈值，In other words, it repeats until the centroids do not move significantly。也可以用质心移动的幅度来判定。

- 遇到的问题：
  - 可能会陷入local minimum，极大的依赖于初始点的选择位置，因此人们经常多次选择inertial centroids，“k-means++”参数被sklearn所选择，它选择初始质心点让它们的距离较远。
  - 提供了样本的权重设置，适用于有重复的样本点，For example, assigning a weight of 2 to a sample is equivalent to adding a duplicate of that sample to the dataset `X`
  - 可以并行计算，n_jobs去调整

- Mini-batch K-means
  - 聚类步骤与经典算法相同，同样是随机选择中心，每次随机抽取一个batch去更新质心。
  - mini-batch提高了运算速率，但是牺牲了精度

#### Affinity Propagation(AP聚类，近邻传播算法)

AP聚类一般翻译为近邻传播聚类，07年被提出。

> AP算法的基本思想是将全部样本看做网络的节点，然后通过网络中各条边的消息传递计算出个样本的聚类中心。聚类过程中，共有两种消息在各节点间传递，分别是吸引度（responsibility）和归属度（availability）。AP算法通过迭代过程不断更新每一个点的吸引度和归属度，直到产生m个高质量的Exemplar（相当于质心），同时将其余的数据点分配到相应的聚类中。

> 关于其算法流程，知乎上kael 用户将AP聚类过程比喻为选举过程： 
> * 所有人都参加选举（大家都是选民也都是参选人），要选出几个作为代表 
> * s(i,k)就相当于i对选k这个人的一个固有的偏好程度 
> * r(i,k)表示用s(i,k)减去最强竞争者的评分，可以理解为k在对i这个选民的竞争中的优势程度 
> * r(i,k)的更新过程对应选民i对各个参选人的挑选（越出众越有吸引力） 
> * a(i,k)：从公式里可以看到，所有r(i’,k)>0的值都对a有正的加成。对应到我们这个比喻中，就相当于选民i通过网上关于k的民意调查看到：有很多人（即i’们）都觉得k不错（r(i’,k)>0），那么选民i也就会相应地觉得k不错，是个可以相信的选择 
> * a(i,k)的更新过程对应关于参选人k的民意调查对于选民i的影响（已经有了很多跟随者的人更有吸引力） 
> * 两者交替的过程也就可以理解为选民在各个参选人之间不断地比较和不断地参考各个参选人给出的民意调查。 
> * r(i,k)的思想反映的是竞争，a(i,k)则是为了让聚类更成功。
> ————————————————
> 版权声明：本文为CSDN博主「winkake」的原创文章，遵循 CC 4.0 BY-SA 版权协议，转载请附上原文出处链接及本声明。
> 原文链接：https://blog.csdn.net/u010161379/article/details/51636926

该算法用了两个指标，一个是点本身对中心点的信任程度`r(i,k)`,另外一个是点依照其他点对该中心点的信任程度作为度量a(i,k)